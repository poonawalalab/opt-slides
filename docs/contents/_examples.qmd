# Examples

## Convex Problems {.smaller}

:::: {.columns}

::: {.column width="40%"}

::: {.callout-tip .fragment fragment-index=1}
## A quadratic program (QP)

$$
\begin{align}
\operatorname{minimize} & f(x) = (x_1 - 1)^2 + (x_2 - 1)^2
\end{align}
$$
:::

:::

::: {.column width="60%"}

::: {.callout-tip .fragment fragment-index=2}
## A linearly constrained quadratic program (QP)

$$
\begin{align}
\operatorname{minimize} & f(x) = (x_1 - 1)^2 + (x_2 - 1)^2 \\
\text{subject to} & x_1 + x_2 = 3.
\end{align}
$$
:::

:::

::::


:::: {.columns}

::: {.column width="40%"}

::: {.callout-tip .fragment fragment-index=1}
## A linear program (LP)

$$
\begin{align}
\operatorname{maximize} & f(x) = 2x_1 + x_2\\
\text{subject to} & x_1 + x_2 \leq 1, \\
& x_1, x_2 \geq 0.
\end{align}
$$

* The optimal solution is $\bm{x}^\ast = (1, 0)$.
:::

:::

::: {.column width="60%"}
![](contents/assets/linear_program_feasible_set.png){fig-align="center" width=60%}
:::

::::


## Nonlinear Program (NLP) {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.callout-tip .fragment fragment-index=1}
## A nonlinear program (NLP)

$$
\begin{align}
\operatorname{maximize} & f(x) = -(x_1+x_2)^2\\
\text{subject to} & x_1x_2 \geq 0, \\
& -2 \leq x_1 \leq 1, \\
& -2 \leq x_2 \leq 1.
\end{align}
$$

:::

:::

::: {.column width="50%"}
![](contents/assets/nonlinear_program.png){fig-align="center" width=80%}
:::

::::


* The point $\bm{x}_c = (1,1)$ has an objective value $f(\bm{x}_c) = 4$, which
is higher than any of its "nearby" feasible points (_local optimizer_).

* In contrast, the point $\bm{x}^\ast = (-2, -2)$ has an objective value
$f(\bm{x}^\ast) = 16$, which is the best among all feasible points (_global
optimizer_).


## Test

$$
\begin{align}
\operatorname{minimize} & f(x) = (0.5 x_1 - 2)^2 + (x_2 - 3)^2
\end{align}
$$

::: {.center}

```{python}

import numpy as np
import matplotlib.pyplot as plt

# Define the objective function and its gradient and Hessian
def f(x, y):
    return (0.5*x - 2)**2 + (y - 3)**2

def grad_f(x, y):
    """Gradient of f"""
    grad_x = 2 * (0.5*x - 2)*0.5
    grad_y = 2 * (y - 3)
    return np.array([grad_x, grad_y])

def hess_f(x, y):
    """Hessian of f"""
    return np.array([[1, 0],
                     [0, 2]])

# Newton's Method
def newtons_method(start, tol=1e-6, max_iter=50):
    x = np.array(start, dtype=float)
    iterates = [x.copy()]
    
    for _ in range(max_iter):
        grad = grad_f(x[0], x[1])
        hess = hess_f(x[0], x[1])
        
        # Newton step: x_new = x - H_inv * grad
        step = np.linalg.solve(hess, grad)
        x -= step
        
        iterates.append(x.copy())
        
        # Convergence check
        if np.linalg.norm(step) < tol:
            break
    
    return x, iterates

# Run Newton's method
start_point = [-1.0, 1.0]
optimum, iterates = newtons_method(start_point)

# Extract iterate points for plotting
iterates = np.array(iterates)
x_iterates, y_iterates = iterates[:, 0], iterates[:, 1]

# Plotting
x = np.linspace(-1, 4, 100)
y = np.linspace(-1, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

plt.figure(figsize=(8, 6))
# Contour plot of the objective function
contour = plt.contour(X, Y, Z, levels=30, cmap="viridis")
plt.clabel(contour, inline=True, fontsize=8)
plt.colorbar(contour, label="Objective Function Value")

# Plot the iterates
plt.plot(x_iterates, y_iterates, 'o-', color="red", label="Iterates")
plt.plot(4, 3, 'x', color="blue", markersize=10, label="True Optimum (4, 3)")

# Annotate start and end points
plt.annotate("Start", (x_iterates[0], y_iterates[0]), textcoords="offset points", xytext=(-10, 10), ha="center", color="red")
plt.annotate("End", (x_iterates[-1], y_iterates[-1]), textcoords="offset points", xytext=(-10, -15), ha="center", color="red")

# Labels and legend
plt.xlabel("x")
plt.ylabel("y")
plt.title("Newton's Method for Optimization")
plt.legend()
plt.grid(True)
plt.show()

```
:::
